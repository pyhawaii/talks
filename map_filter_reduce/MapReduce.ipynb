{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map, Filter, and Reduce with Python\n",
    "\n",
    "Map, filter, and reduce are python primitive functions that give you the ability to quickly process large, sequential data sets (lists, for instance). These functions are also the foundation of processing big data in a distributed environment. Familiarity with map and reduce, especially, is important for understanding efficient data processing.\n",
    "\n",
    "In this tutorial, we will explore the base functions to understand how to use them alone and in conjunction with each other.\n",
    "\n",
    "\n",
    "## Get the data\n",
    "\n",
    "We will be exploring movie data (since I *love* movies).\n",
    "\n",
    "Download the Kaggle [IMDB dataset](https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset).\n",
    "\n",
    "The data has the following fields (in order):\n",
    "\n",
    "1. `color`\n",
    "1. `director_name`\n",
    "1. `num_critic_for_reviews`\n",
    "1. `duration`\n",
    "1. `director_facebook_likes`\n",
    "1. `actor_3_facebook_likes`\n",
    "1. `actor_2_name`\n",
    "1. `actor_1_facebook_likes`\n",
    "1. `gross`\n",
    "1. `genres`\n",
    "1. `actor_1_name`\n",
    "1. `movie_title`\n",
    "1. `num_voted_users`\n",
    "1. `cast_total_facebook_likes`\n",
    "1. `actor_3_name`\n",
    "1. `facenumber_in_poster`\n",
    "1. `plot_keywords`\n",
    "1. `movie_imdb_link`\n",
    "1. `num_user_for_reviews`\n",
    "1. `language`\n",
    "1. `country`\n",
    "1. `content_rating`\n",
    "1. `budget`\n",
    "1. `title_year`\n",
    "1. `actor_2_facebook_likes`\n",
    "1. `imdb_score`\n",
    "1. `aspect_ratio`\n",
    "1. `movie_facebook_likes`\n",
    "\n",
    "Download the file and unzip it into the same directory this notebook is in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data\n",
    "\n",
    "To load the data into your python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('movie_metadata.csv') as file:\n",
    "    data = [ line.strip().split(',') for line in file.readlines()[1:] ]\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping\n",
    "\n",
    "The `map()` function takes two arguments, a function to apply to a set of data and the data to apply the function to. Mapping is similar to a `select` statement in SQL, including choosing which fields to keep for each row and specifying modifications to the raw data.\n",
    "\n",
    "Let's say we want to retrieve the title and year of each movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_title_and_year = map(lambda x: (x[11].decode(\"ascii\" , \"ignore\"), x[23]), data)\n",
    "\n",
    "len(movie_title_and_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_title_and_year[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "The `filter()` function takes two arguments, a function to apply to a set of data and the data to apply the function to. Filtering is similar to the `where` clause in a SQL `select` statement.\n",
    "\n",
    "Let's say that we want to find all movies that are directed by James Cameron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "james_cameron_movies = filter(lambda x: x[1] == 'James Cameron', data)\n",
    "\n",
    "len(james_cameron_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "james_cameron_movies[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing\n",
    "\n",
    "The `reduce()` function takes three arguments, a function to apply an aggregation to a set of data, the data to apply the function to, and an (optionial) initialization value. This is usually the hardest function for people to wrap their heads around, but it is simple if you let it be :)\n",
    "\n",
    "Let's say we want to count the number of movies in the list (I know, you could just use `len(data)`, but this will be much more useful in later examples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = reduce(lambda x, y: x+1, data, 0)\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction is traversing the data one element at a time and applying the function to the previous result (the *accumulated* value) and the current value. In the previous case, we start with the value `0`, and for each element in data we add `1` to the accumulator. `x` in the lambda expression is the accumulated value (which we initialized to `0`) and `y` is the value of each row as it is processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little bit about generators\n",
    "\n",
    "To understand the true power of filter/map/reduce, you need to know about *generators*. This is a topic in and of itself, but basically a generator is a function that processes only as much as it needs to in order to emit an interim value and then stops until more data is requested by the consumer.\n",
    "\n",
    "**Note**: That's **way** over-simplified, for a good overview see [Generators](https://wiki.python.org/moin/Generators) in the [Python Wiki](https://wiki.python.org/). If you take it on faith that generators allow you to keep the minimum amount of data in memory at any point in time, you will be OK for the rest of this discussion.\n",
    "\n",
    "The `map()`, `filter()`, and `reduce()` functions are implemented to use generators to pipeline data through your select/aggregate very efficiently. Intermediate values are created as necessary but released for garbage collection immediately after consumption, so these values are not a bottleneck for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "### Filter and map\n",
    "\n",
    "The first combination of functions that makes sense is applying a function to a sub-set of your data. The pattern is:\n",
    "\n",
    "    map(mapfn, filter(filterfn, data))\n",
    "\n",
    "To get the title of all James Cameron movies, you would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(lambda x: x[11].decode(\"ascii\" , \"ignore\"), filter(lambda x: x[1] == 'James Cameron', data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map and reduce\n",
    "\n",
    "The next combination of functions that makes sense is to aggregate one or more values for each element in a dataset. The pattern is:\n",
    "\n",
    "    reduce(reducefn, map(mapfn, data))\n",
    "\n",
    "To get the total number of genres (counting duplicates) for all movies in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reduce(lambda x, y: x+y, map(lambda x: len(x[9].split('|')), data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and map and reduce\n",
    "\n",
    "The complete/canonical pattern for all uses is:\n",
    "\n",
    "    reduce(fn, map(fn, filter(fn, data)))\n",
    "\n",
    "That's a reduction of a mapping of a filtering of a set of data.\n",
    "\n",
    "Let's say you want to compute the average budget of all films from James Cameron (note that we are assuming a budget of `0` if no budget was provided):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum_num = reduce(\n",
    "    lambda x, y: (x[0]+y, x[1]+1),\n",
    "    map(\n",
    "        lambda x: int(x[22]) if x[22].isdigit() else 0,\n",
    "        filter(\n",
    "            lambda x: x[1] == 'James Cameron',\n",
    "            data)\n",
    "    ),\n",
    "    (0, 0)\n",
    ")\n",
    "\n",
    "sum_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this gives us a total and a count, from which we can compute the average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "round(float(sum_num[0]) / sum_num[1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "\n",
    "Everything here can also be done with `while` or `for` loops around if statements, but MapReduce algorithms are able to take advantage of very powerful optimizations that are not generally available using more verbose constructs.\n",
    "\n",
    "For instance, if we are aggregating data from a CSV file that is multiple terabytes in size, we can use the `csv` library to read and process one line at a time, and `map()`, `filter()`, and `reduce()` this data with almost no overhead. This means we can run meaningful analyses of datasets that are far larger than the resources we have in the machine doing the computations.\n",
    "\n",
    "This would look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('movie_metadata.csv') as file:\n",
    "    sum_num = reduce(\n",
    "        lambda x, y: (x[0]+y, x[1]+1),\n",
    "        map(\n",
    "            lambda x: int(x[22]) if x[22].isdigit() else 0,\n",
    "            filter(\n",
    "                lambda x: x[1] == 'James Cameron',\n",
    "                csv.reader(iter(file.readline, ''))\n",
    "            )\n",
    "        ),\n",
    "        (0, 0)\n",
    "    )\n",
    "\n",
    "sum_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed MapReduce\n",
    "\n",
    "As an extension of that example, if the data itself is spread across multiple machines, the `map()` and `filter()` steps can be performed where the data is (send the code to the data rather than sending the data to the code) and the only thing that happens on the calling machine is the final reduction. This is how Google indexes the entire internet, for instance. But that's a talk for another time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
